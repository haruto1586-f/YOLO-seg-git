import os
import glob
import gc
import torch
import re
from pathlib import Path
from ultralytics import YOLO

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from configs import crack as cfg
import data_manager as dm
import result_saver  # æ¨è«– & ä¿å­˜
import analyzer      # è§£æ & CSVä½œæˆ

def get_sorted_epoch_weights(weights_dir):
    """é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¨ãƒãƒƒã‚¯ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆã—ã¦å–å¾—ã™ã‚‹"""
    files = glob.glob(os.path.join(weights_dir, "epoch_*.pt"))
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡ºã—ã¦ã‚½ãƒ¼ãƒˆ (epoch_1.pt, epoch_2.pt, ... epoch_10.pt)
    def extract_epoch(path):
        match = re.search(r'epoch_(\d+)', os.path.basename(path))
        return int(match.group(1)) if match else 0
    
    return sorted(files, key=extract_epoch)

def analyze_single_experiment(exp_dir):
    """
    1ã¤ã®å®Ÿé¨“ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦ã€æ¨è«–ã¨è§£æ(CSVä½œæˆ)ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
    """
    dir_name = os.path.basename(exp_dir)
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ Analyzing Experiment: {dir_name}")
    print(f"{'='*60}")

    # --- ãƒ‘ã‚¹è¨­å®š ---
    weights_dir = os.path.join(exp_dir, 'weights_history')
    raw_data_dir = os.path.join(exp_dir, 'raw_epoch_data')
    
    # ãƒ•ãƒ«ã‚µã‚¤ã‚ºç‰ˆã®çµæœCSVãƒ‘ã‚¹
    analysis_csv_path = os.path.join(exp_dir, 'detailed_analysis_full.csv')

    # --- äº‹å‰ãƒã‚§ãƒƒã‚¯ ---
    if os.path.exists(analysis_csv_path):
        print(f"âœ… Already analyzed. Skipping: {dir_name}")
        return

    if not os.path.exists(weights_dir):
        print(f"âŒ Weights directory not found: {weights_dir}")
        return

    # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«(å­¦ç¿’/æœªå­¦ç¿’ã®åˆ†å‰²æƒ…å ±)ã®èª­ã¿è¾¼ã¿
    data_pools = dm.load_data_pools(exp_dir)
    if data_pools is None:
        print("âŒ data_pools.json not found. Cannot analyze.")
        return

    # =========================================================
    # Phase 1: æ¨è«–å®Ÿè¡Œ & ç”Ÿãƒ‡ãƒ¼ã‚¿(TXT/NPZ)ä¿å­˜
    # =========================================================
    weight_files = get_sorted_epoch_weights(weights_dir)
    
    if not weight_files:
        print("âŒ No weight files found.")
        return

    print(f"  Found {len(weight_files)} epochs to process.")

    for weight_path in weight_files:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚¨ãƒãƒƒã‚¯ç•ªå·ã‚’å–å¾—
            stem = Path(weight_path).stem  # epoch_10
            epoch = int(stem.split('_')[1])
        except:
            continue
            
        # å¿…è¦ãªSplitï¼ˆTrain/Val/Test/Poolï¼‰ãŒå…¨ã¦å‡¦ç†æ¸ˆã¿ã‹ç¢ºèª
        target_splits = [s for s in ['train', 'val', 'test', 'train_pool'] if s in data_pools]
        all_splits_done = True
        for split in target_splits:
            if not os.path.exists(os.path.join(raw_data_dir, f"epoch_{epoch}_{split}")):
                all_splits_done = False
                break
        
        if all_splits_done:
            print(f"    Epoch {epoch}: Raw data exists. Skipping inference.")
            continue

        print(f"    Epoch {epoch}: Running inference...")
        
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            model = YOLO(weight_path, task='segment')
            
            for split in target_splits:
                images = data_pools[split]
                if not images: continue
                
                # result_saver ã‚’å‘¼ã³å‡ºã—ã¦æ¨è«–çµæœã‚’ä¿å­˜
                # (ä»¥å‰ã®ã‚³ãƒ¼ãƒ‰ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãã®ã¾ã¾ä½¿ç”¨)
                result_saver.save_epoch_predictions(
                    model, epoch, split, images, raw_data_dir
                )
        except Exception as e:
            print(f"    âŒ Error processing epoch {epoch}: {e}")
            continue
        finally:
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            if 'model' in locals(): del model
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    # =========================================================
    # Phase 2: è§£æå®Ÿè¡Œ & CSVä¿å­˜
    # =========================================================
    print(f"\n  Generating Full Analysis CSV...")
    
    try:
        # (analyzer.py ã« run_analysis_full ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å‰æ)
        if hasattr(analyzer, 'run_analysis_full'):
            analyzer.run_analysis_full(
                raw_data_dir, 
                data_pools, 
                analysis_csv_path, 
                img_size=(cfg.IMG_SIZE, cfg.IMG_SIZE)
            )
        else:
            # ä¸‡ãŒä¸€é–¢æ•°åãŒç•°ãªã‚‹å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            print("  Warning: 'run_analysis_full' not found. Trying 'run_analysis'...")
            analyzer.run_analysis(
                raw_data_dir, 
                data_pools, 
                analysis_csv_path, 
                img_size=(cfg.IMG_SIZE, cfg.IMG_SIZE)
            )
            
    except Exception as e:
        print(f"âŒ Analysis failed for {dir_name}: {e}")
        import traceback
        traceback.print_exc()

def main():
    print(f"--- ğŸ“Š Batch Analysis Auto-Detection Mode ---")
    
    # runsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥ä¸‹ã® "subset_*" ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã™ã¹ã¦æ¤œç´¢
    search_pattern = os.path.join(cfg.AL_RUNS_DIR, "subset_*")
    exp_dirs = glob.glob(search_pattern)
    
    # ãƒ•ã‚©ãƒ«ãƒ€ã®ã¿ã‚’æŠ½å‡ºã—ã¦ã‚½ãƒ¼ãƒˆ
    exp_dirs = sorted([d for d in exp_dirs if os.path.isdir(d)])
    
    if not exp_dirs:
        print(f"âŒ No 'subset_*' experiment folders found in: {cfg.AL_RUNS_DIR}")
        return

    print(f"Found {len(exp_dirs)} experiments to analyze.")
    
    # å„å®Ÿé¨“ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦è§£æã‚’å®Ÿè¡Œ
    for exp_dir in exp_dirs:
        analyze_single_experiment(exp_dir)

    print("\nâœ… All batch analyses completed.")

if __name__ == "__main__":
    main()